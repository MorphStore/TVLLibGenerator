register_size 128
{
  base_types = {"uint64_t", "uint32_t", "uint16_t", "uint8_t", "float", "double"}
  # provide one implementation for each base_type, use the same sequence as for base_types
  implementations = {
  "return _mm_extract_epi64( _mm_castpd_si128( _mm_hadd_pd( _mm_castsi128_pd(p_vec1), _mm_castsi128_pd(p_vec1) ) ), 0 );",
  "__m128i tmp = _mm_castps_si128( _mm_hadd_ps( _mm_castsi128_ps(p_vec1), _mm_castsi128_ps(p_vec1) ) ); return _mm_extract_epi32(tmp,0)+_mm_extract_epi32(tmp,1);",
  "return _mm_extract_epi16(p_vec1,0)+_mm_extract_epi16(p_vec1,1)+_mm_extract_epi16(p_vec1,2)+_mm_extract_epi16(p_vec1,3)+_mm_extract_epi16(p_vec1,4)+_mm_extract_epi16(p_vec1,5)+_mm_extract_epi16(p_vec1,6)+_mm_extract_epi16(p_vec1,7);",
  "return _mm_extract_epi8(p_vec1,0)+_mm_extract_epi8(p_vec1,1)+_mm_extract_epi8(p_vec1,2)+_mm_extract_epi8(p_vec1,3)+_mm_extract_epi8(p_vec1,4)+_mm_extract_epi8(p_vec1,5)+_mm_extract_epi8(p_vec1,6)+_mm_extract_epi8(p_vec1,7)+_mm_extract_epi8(p_vec1,8)+_mm_extract_epi8(p_vec1,9)+_mm_extract_epi8(p_vec1,10)+_mm_extract_epi8(p_vec1,11)+_mm_extract_epi8(p_vec1,12)+_mm_extract_epi8(p_vec1,13)+_mm_extract_epi8(p_vec1,14)+_mm_extract_epi8(p_vec1,15);",
  "__m128d tmp = _mm_hadd_ps(p_vec1, p_vec1); return __mm_cvtss_f32(tmp) + _mm_cvtss_f32(_mm_shuffle_ps(tmp, tmp, 0b11100010));",
  "return _mm_cvtsd_f64( _mm_hadd_pd( p_vec1, p_vec1 ) );"
  }
  # number of specialized template parameters exclusing the processing style
  nr_additional_template_parameters = 1
  # additional template specializations. If there is more than 1 template argument and more than 1 base type, follow this sequence: {base type 1/argument1, base type 1/argument 2, ..., base type n/argument 1, base type n/argument 2,..., base type n/argument m}
  template_parameters = {"64", "32", "16", "8","32","64"}
}

register_size 256
{
  base_types = {"uint64_t", "uint32_t", "uint16_t", "uint8_t", "float", "double"}
  # provide one implementation for each base_type, use the same sequence as for base_types
  implementations = {"__m256i tmp = _mm256_castpd_si256( _mm256_hadd_pd( _mm256_castsi256_pd(p_vec1), _mm256_castsi256_pd(p_vec1) ) ); return _mm256_extract_epi64(tmp,0)+_mm256_extract_epi64(tmp,2);",
  "__m256i tmp = _mm256_castps_si256( _mm256_hadd_ps( _mm256_castsi256_ps(p_vec1), _mm256_castsi256_ps(p_vec1) ) ); return _mm256_extract_epi32(tmp,0)+_mm256_extract_epi32(tmp,1)+_mm256_extract_epi32(tmp,4)+_mm256_extract_epi32(tmp,5);",
  "return _mm256_extract_epi16(p_vec1,0)+_mm256_extract_epi16(p_vec1,1)+_mm256_extract_epi16(p_vec1,2)+_mm256_extract_epi16(p_vec1,3)+_mm256_extract_epi16(p_vec1,4)+_mm256_extract_epi16(p_vec1,5)+_mm256_extract_epi16(p_vec1,6)+_mm256_extract_epi16(p_vec1,7)+_mm256_extract_epi16(p_vec1,8)+_mm256_extract_epi16(p_vec1,9)+_mm256_extract_epi16(p_vec1,10)+_mm256_extract_epi16(p_vec1,11)+_mm256_extract_epi16(p_vec1,12)+_mm256_extract_epi16(p_vec1,13)+_mm256_extract_epi16(p_vec1,14)+_mm256_extract_epi16(p_vec1,15);",
  " return _mm256_extract_epi8(p_vec1,0)+_mm256_extract_epi8(p_vec1,1)+_mm256_extract_epi8(p_vec1,2)+_mm256_extract_epi8(p_vec1,3)+_mm256_extract_epi8(p_vec1,4)+_mm256_extract_epi8(p_vec1,5)+_mm256_extract_epi8(p_vec1,6)+_mm256_extract_epi8(p_vec1,7)+   _mm256_extract_epi8(p_vec1,8)+_mm256_extract_epi8(p_vec1,9)+_mm256_extract_epi8(p_vec1,10)+_mm256_extract_epi8(p_vec1,11)+_mm256_extract_epi8(p_vec1,12)+_mm256_extract_epi8(p_vec1,13)+_mm256_extract_epi8(p_vec1,14)+_mm256_extract_epi8(p_vec1,15)+_mm256_extract_epi8(p_vec1,16)+_mm256_extract_epi8(p_vec1,17)+_mm256_extract_epi8(p_vec1,18)+_mm256_extract_epi8(p_vec1,19)+_mm256_extract_epi8(p_vec1,20)+_mm256_extract_epi8(p_vec1,21)+_mm256_extract_epi8(p_vec1,22)+_mm256_extract_epi8(p_vec1,23+_mm256_extract_epi8(p_vec1,24)+_mm256_extract_epi8(p_vec1,25)+_mm256_extract_epi8(p_vec1,26)+_mm256_extract_epi8(p_vec1,27)+_mm256_extract_epi8(p_vec1,28)+_mm256_extract_epi8(p_vec1,29)+_mm256_extract_epi8(p_vec1,30)+_mm256_extract_epi8(p_vec1,31);",

  "__m256 tmp = _mm256_hadd_ps(p_vec1, p_vec1);tmp = _mm256_hadd_ps(tmp, tmp);return __mm256_cvtsd_f64(tmp)+_mm256_cvtsd_f64(_mm256_permute2f128_pd (tmp, tmp, 0b01000001));",

  "__m256d tmp = _mm256_hadd_pd(p_vec1, p_vec1); return __mm256_cvtsd_f64(tmp)+_mm256_cvtsd_f64(_mm256_permute2f128_pd (tmp, tmp, 0b01000001));"

    }
  # number of specialized template parameters exclusing the processing style
  nr_additional_template_parameters = 1
  # additional template specializations. If there is more than 1 template argument and more than 1 base type, follow this sequence: {base type 1/argument1, base type 1/argument 2, ..., base type n/argument 1, base type n/argument 2,..., base type n/argument m}
  template_parameters = {"64", "32", "16", "8","32","64"}
}
