register_size 512
{
  base_types = {"uint64_t", "uint32_t", "uint16_t", "uint8_t"}
  # provide one implementation for each base_type, use the same sequence as for base_types
  implementations = {"return _mm512_reduce_add_epi64(p_vec1);","return _mm512_reduce_add_epi32(p_vec1);",
  "return _mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),0)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),1)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),2)+
         _mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),3)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),4)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),5)+
         _mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),6)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),7)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),8)+
         _mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),9)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),10)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),11)+
         _mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),12)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),13)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),14)+
         _mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,0),15)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),0)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),1)+
         _mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),2)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),3)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),4)+
         _mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),5)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),6)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),7)+
         _mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),8)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),9)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),10)+
         _mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),11)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),12)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),13)+
         _mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),14)+_mm256_extract_epi16(_mm512_extracti64x4_epi64(p_vec1,1),15);",
         "         return _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),0)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),1)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),2)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),3)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),4)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),5)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),6)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),7)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),8)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),9)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),10)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),11)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),12)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),13)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),14)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),15)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),16)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),17)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),18)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),19)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),20)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),21)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),22)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),23)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),24)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),25)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),26)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),27)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),28)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),29)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),30)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,0),31)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),0)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),1)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),2)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),3)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),4)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),5)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),6)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),7)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),8)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),9)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),10)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),11)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),12)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),13)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),14)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),15)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),16)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),17)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),18)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),19)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),20)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),21)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),22)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),23)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),24)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),25)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),26)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),27)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),28)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),29)+
         _mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),30)+_mm256_extract_epi8(_mm512_extracti64x4_epi64(p_vec1,1),31);"} 
  # number of specialized template parameters exclusing the processing style
  nr_additional_template_parameters = 1
  # additional template specializations. If there is more than 1 template argument and more than 1 base type, follow this sequence: {base type 1/argument1, base type 1/argument 2, ..., base type n/argument 1, base type n/argument 2,..., base type n/argument m} 
  template_parameters = {"64","32","16","8"}
}

register_size 256
{
  base_types = {"uint64_t", "uint32_t", "uint16_t", "uint8_t"}
  # provide one implementation for each base_type, use the same sequence as for base_types
  implementations = {" return _mm256_extract_epi64(p_vec1, 0) + \
                 _mm256_extract_epi64(p_vec1, 1) + \
                 _mm256_extract_epi64(p_vec1, 2) + \
                 _mm256_extract_epi64(p_vec1, 3);",
                 "__m256i tmp =
            _mm256_castps_si256(
               _mm256_hadd_ps(
                  _mm256_castsi256_ps(p_vec1),
                  _mm256_castsi256_ps(p_vec1)
               )
            );
          return _mm256_extract_epi32(tmp,0)+_mm256_extract_epi32(tmp,1)+_mm256_extract_epi32(tmp,4)+_mm256_extract_epi32(tmp,5);",
  "return _mm256_extract_epi16(p_vec1,0)+_mm256_extract_epi16(p_vec1,1)+_mm256_extract_epi16(p_vec1,2)+_mm256_extract_epi16(p_vec1,3)
         +_mm256_extract_epi16(p_vec1,4)+_mm256_extract_epi16(p_vec1,5)+_mm256_extract_epi16(p_vec1,6)+_mm256_extract_epi16(p_vec1,7)+
         _mm256_extract_epi16(p_vec1,8)+_mm256_extract_epi16(p_vec1,9)+_mm256_extract_epi16(p_vec1,10)+_mm256_extract_epi16(p_vec1,11)
         +_mm256_extract_epi16(p_vec1,12)+_mm256_extract_epi16(p_vec1,13)+_mm256_extract_epi16(p_vec1,14)+_mm256_extract_epi16(p_vec1,15);",
         " return _mm256_extract_epi8(p_vec1,0)+_mm256_extract_epi8(p_vec1,1)+_mm256_extract_epi8(p_vec1,2)+_mm256_extract_epi8(p_vec1,3)
         +_mm256_extract_epi8(p_vec1,4)+_mm256_extract_epi8(p_vec1,5)+_mm256_extract_epi8(p_vec1,6)+_mm256_extract_epi8(p_vec1,7)+
         _mm256_extract_epi8(p_vec1,8)+_mm256_extract_epi8(p_vec1,9)+_mm256_extract_epi8(p_vec1,10)+_mm256_extract_epi8(p_vec1,11)
         +_mm256_extract_epi8(p_vec1,12)+_mm256_extract_epi8(p_vec1,13)+_mm256_extract_epi8(p_vec1,14)+_mm256_extract_epi8(p_vec1,15)+
         _mm256_extract_epi8(p_vec1,16)+_mm256_extract_epi8(p_vec1,17)+_mm256_extract_epi8(p_vec1,18)+_mm256_extract_epi8(p_vec1,19)
         +_mm256_extract_epi8(p_vec1,20)+_mm256_extract_epi8(p_vec1,21)+_mm256_extract_epi8(p_vec1,22)+_mm256_extract_epi8(p_vec1,23)+
         _mm256_extract_epi8(p_vec1,24)+_mm256_extract_epi8(p_vec1,25)+_mm256_extract_epi8(p_vec1,26)+_mm256_extract_epi8(p_vec1,27)
         +_mm256_extract_epi8(p_vec1,28)+_mm256_extract_epi8(p_vec1,29)+_mm256_extract_epi8(p_vec1,30)+_mm256_extract_epi8(p_vec1,31);"} 
  # number of specialized template parameters exclusing the processing style
  nr_additional_template_parameters = 1
  # additional template specializations. If there is more than 1 template argument and more than 1 base type, follow this sequence: {base type 1/argument1, base type 1/argument 2, ..., base type n/argument 1, base type n/argument 2,..., base type n/argument m} 
  template_parameters = {"64","32","16","8"}
}

register_size 128
{
  base_types = {"uint64_t", "uint32_t", "uint16_t", "uint8_t"}
  # provide one implementation for each base_type, use the same sequence as for base_types
  implementations = {" return _mm_extract_epi64(p_vec1, 0) + _mm_extract_epi64(p_vec1, 1);",
                 "__m128i tmp =
            _mm_castps_si128(
               _mm_hadd_ps(
                  _mm_castsi128_ps(p_vec1),
                  _mm_castsi128_ps(p_vec1)
               )
            );
         return _mm_extract_epi32(tmp,0)+_mm_extract_epi32(tmp,1);",
  " return _mm_extract_epi16(p_vec1,0)+_mm_extract_epi16(p_vec1,1)+_mm_extract_epi16(p_vec1,2)+_mm_extract_epi16(p_vec1,3)
         +_mm_extract_epi16(p_vec1,4)+_mm_extract_epi16(p_vec1,5)+_mm_extract_epi16(p_vec1,6)+_mm_extract_epi16(p_vec1,7);",
         "  return _mm_extract_epi8(p_vec1,0)+_mm_extract_epi8(p_vec1,1)+_mm_extract_epi8(p_vec1,2)+_mm_extract_epi8(p_vec1,3)
         +_mm_extract_epi8(p_vec1,4)+_mm_extract_epi8(p_vec1,5)+_mm_extract_epi8(p_vec1,6)+_mm_extract_epi8(p_vec1,7)+
         _mm_extract_epi8(p_vec1,8)+_mm_extract_epi8(p_vec1,9)+_mm_extract_epi8(p_vec1,10)+_mm_extract_epi8(p_vec1,11)
         +_mm_extract_epi8(p_vec1,12)+_mm_extract_epi8(p_vec1,13)+_mm_extract_epi8(p_vec1,14)+_mm_extract_epi8(p_vec1,15);"} 
  # number of specialized template parameters exclusing the processing style
  nr_additional_template_parameters = 1
  # additional template specializations. If there is more than 1 template argument and more than 1 base type, follow this sequence: {base type 1/argument1, base type 1/argument 2, ..., base type n/argument 1, base type n/argument 2,..., base type n/argument m} 
  template_parameters = {"64","32","16","8"}
}