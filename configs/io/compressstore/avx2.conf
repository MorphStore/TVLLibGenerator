register_size 256
{
  base_types = {"uint64_t","uint64_t"}
  # provide one implementation for each base_type, use the same sequence as for base_types
  implementations = { "
          switch (mask){
            case 0: break;
            //                    case 0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
            //                    case 1: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
            case 1: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), p_vec); break;
            case 2: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
            //                    case 3: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
            case 3: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), p_vec); break;
            case 4: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,78)); break;
            case 5: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,216)); break;
            case 6: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
            //                    case 7: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
            case 7: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), p_vec); break;
            case 8: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,147)); break;
            case 9: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,156)); break;
            case 10: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,141)); break;
            case 11: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,180)); break;
            case 12: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,78)); break;
            case 13: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,120)); break;
            case 14: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,57)); break;
            case 15: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),p_vec); break;
         }
         return ;",
	"switch (mask){
            case 0: break;
            //                    case 0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
            //                    case 1: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
            case 1: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), p_vec); break;
            case 2: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
            //                    case 3: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
            case 3: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), p_vec); break;
            case 4: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,78)); break;
            case 5: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,216)); break;
            case 6: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
            //                    case 7: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
            case 7: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), p_vec); break;
            case 8: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,147)); break;
            case 9: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,156)); break;
            case 10: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,141)); break;
            case 11: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,180)); break;
            case 12: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,78)); break;
            case 13: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,120)); break;
            case 14: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,57)); break;
            case 15: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),p_vec); break;
         }
         return ;" }
  # number of specialized template parameters exclusing the processing style
  nr_additional_template_parameters = 2
  # additional template specializations. If there is more than 1 template argument and more than 1 base type, follow this sequence: {base type 1/argument1, base type 1/argument 2, ..., base type n/argument 1, base type n/argument 2,..., base type n/argument m}
  template_parameters = {"iov::UNALIGNED","64","iov::ALIGNED","64"}
}
