register_size 128
{
  #SAME implementation as SSE.conf
  base_types = {"uint64_t","uint64_t", "uint32_t", "uint32_t", "uint16_t", "uint16_t", "uint8_t", "uint8_t"}
  # provide one implementation for each base_type, use the same sequence as for base_types
  implementations = {
    #64 Bit int
    "switch (mask){
    case 0:   return; //store nothing
    case 1:   _mm_storeu_si128(reinterpret_cast<typename sse< v128< uint64_t > >::vector_t  *>(p_DataPtr),p_vec);
              return; //store everything
    case 2:   p_vec=_mm_shuffle_epi8(p_vec, _mm_set_epi8(7,6,5,4,3,2,1,0,15,14,13,12,11,10,9,8)); //move upper 64 bit to beginning of register and store it to memory
              _mm_storeu_si128(reinterpret_cast<typename sse< v128< uint64_t > >::vector_t  *>(p_DataPtr),p_vec);
              return;
    case 3:   _mm_storeu_si128(reinterpret_cast<typename sse< v128< uint64_t > >::vector_t  *>(p_DataPtr),p_vec); //store everything
              return;
    }
    return ;",

    "switch (mask){
    case 0:   return; //store nothing
    case 1:   _mm_storeu_si128(reinterpret_cast<typename sse< v128< uint64_t > >::vector_t  *>(p_DataPtr),p_vec);
              return; //store everything
    case 2:   p_vec=_mm_shuffle_epi8(p_vec, _mm_set_epi8(7,6,5,4,3,2,1,0,15,14,13,12,11,10,9,8)); //move upper 64 bit to beginning of register and store it to memory
              _mm_storeu_si128(reinterpret_cast<typename sse< v128< uint64_t > >::vector_t  *>(p_DataPtr),p_vec);
              return;
    case 3:   _mm_storeu_si128(reinterpret_cast<typename sse< v128< uint64_t > >::vector_t  *>(p_DataPtr),p_vec); //store everything
              return;
    }
    return ;",
    #32 Bit int
    "switch (mask){
        case 0b0000: return;
        case 0b0001: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),p_vec); return;
        case 0b0010: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_srli_si128(p_vec,4)); return;
        case 0b0011: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),p_vec); return;
        case 0b0100: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_srli_si128(p_vec,8)); return;
        case 0b0101: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_shuffle_epi8(p_vec,_mm_set_epi8(15, 14, 13, 12, 7, 6, 5, 4, 11, 10, 9, 8, 3, 2, 1, 0))); return;
        case 0b0110: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_srli_si128(p_vec,4)); return;
        case 0b0111: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),p_vec); return;
        case 0b1000: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_srli_si128(p_vec,12)); return;
        case 0b1001: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_shuffle_epi8(p_vec,_mm_set_epi8(11, 10, 9, 8, 7, 6, 5, 4, 15, 14, 13, 12, 3, 2, 1, 0))); return;
        case 0b1010: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_shuffle_epi8(p_vec,_mm_set_epi8(11, 10, 9, 8, 3, 2, 1, 0, 15, 14, 13, 12, 7, 6, 5, 4))); return;
        case 0b1011: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_shuffle_epi8(p_vec,_mm_set_epi8(11, 10, 9, 8, 15, 14, 13, 12, 7, 6, 5, 4, 3, 2, 1, 0))); return;
        case 0b1100: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_srli_si128(p_vec,8)); return;
        case 0b1101: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_shuffle_epi8(p_vec,_mm_set_epi8(7, 6, 5, 4, 15, 14, 13, 12, 11, 10, 9, 8, 3, 2, 1, 0))); return;
        case 0b1110: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_srli_si128(p_vec,4)); return;
        case 0b1111: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),p_vec); return;
     }
     return ;",

     "switch (mask){
         case 0b0000: return;
         case 0b0001: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),p_vec); return;
         case 0b0010: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_srli_si128(p_vec,4)); return;
         case 0b0011: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),p_vec); return;
         case 0b0100: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_srli_si128(p_vec,8)); return;
         case 0b0101: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_shuffle_epi8(p_vec,_mm_set_epi8(15, 14, 13, 12, 7, 6, 5, 4, 11, 10, 9, 8, 3, 2, 1, 0))); return;
         case 0b0110: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_srli_si128(p_vec,4)); return;
         case 0b0111: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),p_vec); return;
         case 0b1000: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_srli_si128(p_vec,12)); return;
         case 0b1001: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_shuffle_epi8(p_vec,_mm_set_epi8(11, 10, 9, 8, 7, 6, 5, 4, 15, 14, 13, 12, 3, 2, 1, 0))); return;
         case 0b1010: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_shuffle_epi8(p_vec,_mm_set_epi8(11, 10, 9, 8, 3, 2, 1, 0, 15, 14, 13, 12, 7, 6, 5, 4))); return;
         case 0b1011: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_shuffle_epi8(p_vec,_mm_set_epi8(11, 10, 9, 8, 15, 14, 13, 12, 7, 6, 5, 4, 3, 2, 1, 0))); return;
         case 0b1100: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_srli_si128(p_vec,8)); return;
         case 0b1101: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_shuffle_epi8(p_vec,_mm_set_epi8(7, 6, 5, 4, 15, 14, 13, 12, 11, 10, 9, 8, 3, 2, 1, 0))); return;
         case 0b1110: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),_mm_srli_si128(p_vec,4)); return;
         case 0b1111: _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),p_vec); return;
      }
      return ;",
      #16 Bit int
      "int8_t matched = 0;
       while(mask != 0){
          if((mask & 0x1) == 0x1){
             if(matched == 0){
                _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),p_vec);
                matched = 1;
             }
             p_DataPtr ++;
          }else{
             matched = 0;
          }
          mask = (mask >> 1) & 0x7FFF;
          p_vec = _mm_srli_si128(p_vec,2);
       }
       return ;",

       "int8_t matched = 0;
        while(mask != 0){
           if((mask & 0x1) == 0x1){
              if(matched == 0){
                 _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),p_vec);
                 matched = 1;
              }
              p_DataPtr ++;
           }else{
              matched = 0;
           }
           mask = (mask >> 1) & 0x7FFF;
           p_vec = _mm_srli_si128(p_vec,2);
        }
        return ;",
        #8 Bit int
        "int8_t matched = 0;
         while(mask != 0){
            if((mask & 0x1) == 0x1){
               if(matched == 0){
                  _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),p_vec);
                  matched = 1;
               }
               p_DataPtr ++;
            }else{
               matched = 0;
            }

            mask = (mask >> 1) & 0x7FFF;
            p_vec = _mm_srli_si128(p_vec,1);
         }
         return ;",

         "int8_t matched = 0;
          while(mask != 0){
             if((mask & 0x1) == 0x1){
                if(matched == 0){
                   _mm_storeu_si128(reinterpret_cast<typename sse< v128< U > >::vector_t  *>(p_DataPtr),p_vec);
                   matched = 1;
                }
                p_DataPtr ++;
             }else{
                matched = 0;
             }

             mask = (mask >> 1) & 0x7FFF;
             p_vec = _mm_srli_si128(p_vec,1);
          }
          return ;"
  }
  # number of specialized template parameters exclusing the processing style
  nr_additional_template_parameters = 2
  # additional template specializations. If there is more than 1 template argument and more than 1 base type, follow this sequence: {base type 1/argument1, base type 1/argument 2, ..., base type n/argument 1, base type n/argument 2,..., base type n/argument m}
  template_parameters = {
    "iov::UNALIGNED","64", "iov::ALIGNED","64",
    "iov::UNALIGNED","32", "iov::ALIGNED","32",
    "iov::UNALIGNED","16", "iov::ALIGNED","16",
    "iov::UNALIGNED","8", "iov::ALIGNED","8"
  }
}


register_size 256
{
  base_types = {"uint64_t","uint64_t", "uint32_t", "uint32_t", "uint16_t", "uint16_t", "uint8_t", "uint8_t"}
  # provide one implementation for each base_type, use the same sequence as for base_types
  implementations = {
    #64 Bit int
    "switch (mask){
      case 0: break;
      //                    case 0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
      //                    case 1: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
      case 1: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), p_vec); break;
      case 2: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      //                    case 3: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
      case 3: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), p_vec); break;
      case 4: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,78)); break;
      case 5: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,216)); break;
      case 6: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      //                    case 7: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
      case 7: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), p_vec); break;
      case 8: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,147)); break;
      case 9: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,156)); break;
      case 10: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,141)); break;
      case 11: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,180)); break;
      case 12: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,78)); break;
      case 13: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,120)); break;
      case 14: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,57)); break;
      case 15: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),p_vec); break;
    }
    return ;",
    "switch (mask){
      case 0: break;
      //                    case 0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
      //                    case 1: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
      case 1: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), p_vec); break;
      case 2: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      //                    case 3: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
      case 3: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), p_vec); break;
      case 4: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,78)); break;
      case 5: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,216)); break;
      case 6: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      //                    case 7: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,228)); break;
      case 7: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), p_vec); break;
      case 8: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,147)); break;
      case 9: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,156)); break;
      case 10: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,141)); break;
      case 11: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,180)); break;
      case 12: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,78)); break;
      case 13: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,120)); break;
      case 14: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,57)); break;
      case 15: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< uint64_t > >::vector_t *>(p_DataPtr),p_vec); break;
    }
    return ;",
    #32 Bit int
    "int matched = 1;
    switch (mask){
      case 0x00: break;
      case 0x01: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      case 0x03: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      case 0x07: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      case 0x0C: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      case 0x0F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      case 0x1F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      case 0x30: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,78)); break;
      case 0x33: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,216)); break;
      case 0x3C: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      case 0x3F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      case 0x7F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      case 0xC0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,147)); break;
      case 0xC3: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,156)); break;
      case 0xCC: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,141)); break;
      case 0xCF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,180)); break;
      case 0xF0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,78)); break;
      case 0xF3: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,120)); break;
      case 0xFC: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      case 0xFF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      default: matched = 0; break;
    }
    if(matched == 0){
      while(mask != 0){
         if((mask & 0x1) == 0x1){
            if(matched == 0){
               matched = 1;
               _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec);
            }
            p_DataPtr ++;
         }else{
            matched = 0;
         }
         mask = (mask >> 1) & 0x7FFFFFFF;    //because of sign extention that might happen.
         __m256i srl64_q = _mm256_permute4x64_epi64(p_vec, _MM_SHUFFLE(0,3,2,1));
         __m256i srl64_m = _mm256_slli_epi64(srl64_q, 32);
         __m256i srl8_z = _mm256_srli_epi64(p_vec, 32);
         __m256i srl64 = _mm256_and_si256(srl64_m, _mm256_set_epi64x(0, ~0, ~0, ~0));
         p_vec = _mm256_or_si256(srl64, srl8_z);
      }
    }
    return ;",
    "int matched = 1;
    switch (mask){
      case 0x00: break;
      case 0x01: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      case 0x03: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      case 0x07: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      case 0x0C: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      case 0x0F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      case 0x1F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      case 0x30: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,78)); break;
      case 0x33: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,216)); break;
      case 0x3C: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      case 0x3F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      case 0x7F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      case 0xC0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,147)); break;
      case 0xC3: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,156)); break;
      case 0xCC: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,141)); break;
      case 0xCF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,180)); break;
      case 0xF0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,78)); break;
      case 0xF3: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,120)); break;
      case 0xFC: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      case 0xFF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), p_vec); break;
      default: matched = 0; break;
    }
    if(matched == 0){
      while(mask != 0){
         if((mask & 0x1) == 0x1){
            if(matched == 0){
               matched = 1;
               _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec);
            }
            p_DataPtr ++;
         }else{
            matched = 0;
         }
         mask = (mask >> 1) & 0x7FFFFFFF;    //because of sign extention that might happen.
         __m256i srl64_q = _mm256_permute4x64_epi64(p_vec, _MM_SHUFFLE(0,3,2,1));
         __m256i srl64_m = _mm256_slli_epi64(srl64_q, 32);
         __m256i srl8_z = _mm256_srli_epi64(p_vec, 32);
         __m256i srl64 = _mm256_and_si256(srl64_m, _mm256_set_epi64x(0, ~0, ~0, ~0));
         p_vec = _mm256_or_si256(srl64, srl8_z);
      }
    }
    return ;",
    #16 Bit int
    "int matched = 1;
    switch (mask){
      case 0x0000: break;
      case 0x0001: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0003: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0007: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x000F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x001F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x003F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x007F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00F0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,57)); break;
      case 0x00FF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x01FF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x03FF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x07FF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0F00: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,78)); break;
      case 0x0F0F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,216)); break;
      case 0x0FF0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,57)); break;
      case 0x0FFF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x1FFF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x3FFF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x7FFF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0xF000: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,147)); break;
      case 0xF00F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,156)); break;
      case 0xF0F0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,141)); break;
      case 0xF0FF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,180)); break;
      case 0xFF00: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,78)); break;
      case 0xFF0F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,120)); break;
      case 0xFFF0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,57)); break;
      case 0xFFFF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      default: matched = 0; break;
    }
    if(matched == 0){
      while(mask != 0){
        if((mask & 0x1) == 0x1){
          if(matched == 0){
            matched = 1;
            _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec);
          }
          p_DataPtr ++;
        }else{
          matched = 0;
        }
        mask = (mask >> 1) & 0x7FFFFFFF;
        __m256i srl64_q = _mm256_permute4x64_epi64(p_vec, _MM_SHUFFLE(0,3,2,1));
        __m256i srl64_m = _mm256_slli_epi64(srl64_q, 3*16);
        __m256i srl8_z = _mm256_srli_epi64(p_vec, 1*16);
        __m256i srl64 = _mm256_and_si256(srl64_m, _mm256_set_epi64x(0, ~0, ~0, ~0));
        p_vec = _mm256_or_si256(srl64, srl8_z);
      }
    }
    return ;",
    "int matched = 1;
    switch (mask){
      case 0x0000: break;
      case 0x0001: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0003: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0007: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x000F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x001F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x003F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x007F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00F0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,57)); break;
      case 0x00FF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x01FF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x03FF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x07FF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0F00: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,78)); break;
      case 0x0F0F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,216)); break;
      case 0x0FF0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,57)); break;
      case 0x0FFF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x1FFF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x3FFF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x7FFF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0xF000: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,147)); break;
      case 0xF00F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,156)); break;
      case 0xF0F0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,141)); break;
      case 0xF0FF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,180)); break;
      case 0xFF00: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,78)); break;
      case 0xFF0F: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,120)); break;
      case 0xFFF0: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),_mm256_permute4x64_epi64(p_vec,57)); break;
      case 0xFFFF: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      default: matched = 0; break;
    }
    if(matched == 0){
      while(mask != 0){
        if((mask & 0x1) == 0x1){
          if(matched == 0){
            matched = 1;
            _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec);
          }
          p_DataPtr ++;
        }else{
          matched = 0;
        }
        mask = (mask >> 1) & 0x7FFFFFFF;
        __m256i srl64_q = _mm256_permute4x64_epi64(p_vec, _MM_SHUFFLE(0,3,2,1));
        __m256i srl64_m = _mm256_slli_epi64(srl64_q, 3*16);
        __m256i srl8_z = _mm256_srli_epi64(p_vec, 1*16);
        __m256i srl64 = _mm256_and_si256(srl64_m, _mm256_set_epi64x(0, ~0, ~0, ~0));
        p_vec = _mm256_or_si256(srl64, srl8_z);
      }
    }
    return ;",
    #8 Bit int
    "int matched = 1;
    switch (mask){
      case 0x0: break;
      case 0x00000001: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00000003: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00000007: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0000000f: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0000001f: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0000003f: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0000007f: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x000000ff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x000001ff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x000003ff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x000007ff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00000fff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00001fff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00003fff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00007fff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0000ff00: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      case 0x0000ffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0001ffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0003ffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0007ffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x000fffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x001fffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x003fffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x007fffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00ff0000: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,78)); break;
      case 0x00ff00ff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,216)); break;
      case 0x00ffff00: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      case 0x00ffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x01ffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x03ffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x07ffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0fffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x1fffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x3fffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x7fffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0xff000000: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,147)); break;
      case 0xff0000ff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,159)); break;
      case 0xff00ff00: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,141)); break;
      case 0xff00ffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,180)); break;
      case 0xffff0000: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,78)); break;
      case 0xffff00ff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,120)); break;
      case 0xffffff00: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      case 0xffffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      default: matched = 0; break;
    }
    //shifting over SSE Lanes: https://stackoverflow.com/questions/28664642/avx2-shift-16-bit-integers
    if(matched == 0){
      while(mask != 0){
         if((mask & 0x1) == 0x1){
            if(matched == 0){
               matched = 1;
               _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec);
            }
            p_DataPtr ++;
         }else{
            matched = 0;
         }
         mask = (mask >> 1) & 0x7FFFFFFF;
         __m256i srl64_q = _mm256_permute4x64_epi64(p_vec, _MM_SHUFFLE(0,3,2,1));
         __m256i srl64_m = _mm256_slli_epi64(srl64_q, 7*8);
         __m256i srl8_z = _mm256_srli_epi64(p_vec, 1*8);
         __m256i srl64 = _mm256_and_si256(srl64_m, _mm256_set_epi64x(0, ~0, ~0, ~0));
         p_vec = _mm256_or_si256(srl64, srl8_z);
      }
    }
    return ;",
    "int matched = 1;
    switch (mask){
      case 0x0: break;
      case 0x00000001: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00000003: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00000007: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0000000f: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0000001f: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0000003f: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0000007f: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x000000ff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x000001ff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x000003ff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x000007ff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00000fff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00001fff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00003fff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00007fff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0000ff00: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      case 0x0000ffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0001ffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0003ffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0007ffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x000fffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x001fffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x003fffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x007fffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x00ff0000: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,78)); break;
      case 0x00ff00ff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,216)); break;
      case 0x00ffff00: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      case 0x00ffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x01ffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x03ffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x07ffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x0fffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x1fffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x3fffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0x7fffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      case 0xff000000: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,147)); break;
      case 0xff0000ff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,159)); break;
      case 0xff00ff00: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,141)); break;
      case 0xff00ffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,180)); break;
      case 0xffff0000: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,78)); break;
      case 0xffff00ff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,120)); break;
      case 0xffffff00: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr), _mm256_permute4x64_epi64(p_vec,57)); break;
      case 0xffffffff: _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec); break;
      default: matched = 0; break;
    }
    //shifting over SSE Lanes: https://stackoverflow.com/questions/28664642/avx2-shift-16-bit-integers
    if(matched == 0){
      while(mask != 0){
         if((mask & 0x1) == 0x1){
            if(matched == 0){
               matched = 1;
               _mm256_storeu_si256(reinterpret_cast<typename avx2< v256< U > >::vector_t *>(p_DataPtr),p_vec);
            }
            p_DataPtr ++;
         }else{
            matched = 0;
         }
         mask = (mask >> 1) & 0x7FFFFFFF;
         __m256i srl64_q = _mm256_permute4x64_epi64(p_vec, _MM_SHUFFLE(0,3,2,1));
         __m256i srl64_m = _mm256_slli_epi64(srl64_q, 7*8);
         __m256i srl8_z = _mm256_srli_epi64(p_vec, 1*8);
         __m256i srl64 = _mm256_and_si256(srl64_m, _mm256_set_epi64x(0, ~0, ~0, ~0));
         p_vec = _mm256_or_si256(srl64, srl8_z);
      }
    }
    return ;"
  }
  # number of specialized template parameters exclusing the processing style
  nr_additional_template_parameters = 2
  # additional template specializations. If there is more than 1 template argument and more than 1 base type, follow this sequence: {base type 1/argument1, base type 1/argument 2, ..., base type n/argument 1, base type n/argument 2,..., base type n/argument m}
  template_parameters = {
    "iov::UNALIGNED","64", "iov::ALIGNED","64",
    "iov::UNALIGNED","32", "iov::ALIGNED","32",
    "iov::UNALIGNED","16", "iov::ALIGNED","16",
    "iov::UNALIGNED","8", "iov::ALIGNED","8"
  }
}
